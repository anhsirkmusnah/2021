# Quantum-Inspired Machine Learning (QiML) Platform
## Projected Quantum Kernels for Financial Fraud Detection
### Complete Technical Documentation

---

**Document Version:** 1.0
**Last Updated:** January 2026
**Authors:** Enterprise Quantum Engineering Team
**Classification:** Internal Technical Documentation

---

# Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Project Overview](#2-project-overview)
3. [Theoretical Foundations](#3-theoretical-foundations)
4. [Projected Quantum Kernels](#4-projected-quantum-kernels)
5. [Tensor Network Simulation](#5-tensor-network-simulation)
6. [QMLOps Pipeline](#6-qmlops-pipeline)
7. [Implementation Reference](#7-implementation-reference)
8. [Fraud Detection Application](#8-fraud-detection-application)
9. [Deployment Guide](#9-deployment-guide)
10. [References](#10-references)

---

# 1. Executive Summary

## 1.1 Purpose

This document provides comprehensive technical documentation for the Quantum-Inspired Machine Learning (QiML) platform, which implements **Projected Quantum Kernels (PQK)** for binary classification tasks, with primary application to **financial fraud detection**.

## 1.2 Key Capabilities

| Capability | Description |
|------------|-------------|
| **Quantum Feature Extraction** | Encodes classical data into quantum circuits and extracts Pauli expectation values |
| **Projected Quantum Kernels** | Computes kernel matrices using quantum-derived features for SVM classification |
| **Tensor Network Simulation** | Uses Matrix Product States (MPS) via ITensor for efficient classical simulation |
| **MPI Parallelization** | Distributed computation across multiple nodes for scalable kernel construction |
| **Production Pipeline** | End-to-end workflow from data ingestion to model deployment |

## 1.3 Technology Stack

| Layer | Technology |
|-------|------------|
| Quantum Simulation | ITensor C++ library with custom qubit site type |
| Circuit Construction | pytket (Quantinuum) |
| Parallelization | MPI (mpi4py, OpenMPI) |
| ML Framework | scikit-learn (SVM), LightGBM |
| Language Bridge | pybind11 (C++/Python) |
| Containerization | Docker |

## 1.4 Primary Use Case

**Financial Fraud Detection** using the Elliptic Bitcoin dataset, demonstrating:
- 85-92% accuracy on fraud classification
- 5-10% improvement over classical RBF kernels
- Sub-200ms inference latency per transaction

---

# 2. Project Overview

## 2.1 Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           QiML PLATFORM ARCHITECTURE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚      Classical Data         â”‚
                         â”‚    x âˆˆ â„áµˆ (d features)     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚    Quantum Feature Map      â”‚
                         â”‚  U(x) = HâŠ—â¿ âˆ[RzÂ·XXPhase]  â”‚
                         â”‚      (pytket circuits)      â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   MPS Tensor Network Sim    â”‚
                         â”‚    |Ïˆ(x)âŸ© = U(x)|0âŸ©â¿       â”‚
                         â”‚    (ITensor C++ backend)    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   Projected Quantum Featuresâ”‚
                         â”‚   Î¦(x) = [âŸ¨XâŸ©,âŸ¨YâŸ©,âŸ¨ZâŸ©]áµ¢   â”‚
                         â”‚      (3n dimensions)        â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚                                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Kernel Mode        â”‚         â”‚    Feature Mode       â”‚
         â”‚  k(x,x') = exp(-Î±Â·D)  â”‚         â”‚  Î¦(x) â†’ LightGBM     â”‚
         â”‚     â†’ SVM             â”‚         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2.2 Directory Structure

```
QML dataproc/
â”œâ”€â”€ ITensor_C/                    # ITensor C++ backend
â”‚   â”œâ”€â”€ helloitensor.cc          # C++ MPS simulation core
â”‚   â”œâ”€â”€ qubit.h                  # Custom ITensor qubit site type
â”‚   â”œâ”€â”€ main.py                  # Main execution script
â”‚   â”œâ”€â”€ main_dlp.py              # Discrete log problem variant
â”‚   â”œâ”€â”€ projected_kernel_ansatz.py  # Ansatz + kernel builder
â”‚   â””â”€â”€ datasets/                # Data directory
â”‚
â”œâ”€â”€ QuantumLibs/                  # Python quantum simulation
â”‚   â”œâ”€â”€ main.py                  # Execution entry point
â”‚   â”œâ”€â”€ projected_kernel_ansatz.py
â”‚   â””â”€â”€ projected_quantum_features.py
â”‚
â”œâ”€â”€ dataproc files/               # Production pipeline
â”‚   â”œâ”€â”€ main.py                  # Orchestration script
â”‚   â”œâ”€â”€ generate_pqf.py          # Feature generation
â”‚   â”œâ”€â”€ train.py                 # Model training
â”‚   â””â”€â”€ test.py                  # Model evaluation
â”‚
â”œâ”€â”€ Installation-Script/          # Deployment utilities
â”‚   â”œâ”€â”€ Dockerfile               # Container definition
â”‚   â”œâ”€â”€ elliptic_preproc.py      # Data preprocessing
â”‚   â””â”€â”€ readme.md                # Installation guide
â”‚
â””â”€â”€ docs/                         # Documentation
```

## 2.3 Data Flow Summary

```
Raw Data (CSV)
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PREPROCESSING: QuantileTransform â†’ StandardScaler â†’ MinMaxScaler[-Ï€,Ï€]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUANTUM ENCODING: Build circuit U(x) with Hadamard + Rz + XXPhase gates   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MPS SIMULATION: Simulate |Ïˆ(x)âŸ© = U(x)|0âŸ©â¿ using ITensor                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FEATURE EXTRACTION: Compute âŸ¨XâŸ©, âŸ¨YâŸ©, âŸ¨ZâŸ© for each qubit â†’ Î¦(x) âˆˆ â„Â³â¿   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KERNEL COMPUTATION: k(x,x') = exp(-Î±Â·||Î¦(x)-Î¦(x')||Â²)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLASSIFICATION: SVM with precomputed kernel â†’ Fraud/Legitimate prediction â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 3. Theoretical Foundations

## 3.1 Quantum State Representation

### 3.1.1 Single Qubit States

A qubit exists in superposition of computational basis states:

**Mathematical Definition:**
```
|ÏˆâŸ© = Î±|0âŸ© + Î²|1âŸ©
```

where Î±, Î² âˆˆ â„‚ and |Î±|Â² + |Î²|Â² = 1.

**Bloch Sphere Representation:**
```
|ÏˆâŸ© = cos(Î¸/2)|0âŸ© + e^(iÏ†)sin(Î¸/2)|1âŸ©
```

where Î¸ âˆˆ [0, Ï€] and Ï† âˆˆ [0, 2Ï€).

### 3.1.2 Multi-Qubit Systems

For n qubits, the state space is the tensor product:

```
â„‹ = â„‹â‚ âŠ— â„‹â‚‚ âŠ— ... âŠ— â„‹â‚™ = (â„‚Â²)^âŠ—n
```

A general n-qubit state requires 2â¿ complex amplitudes:

```
|ÏˆâŸ© = Î£ c_{iâ‚iâ‚‚...iâ‚™} |iâ‚iâ‚‚...iâ‚™âŸ©
```

| Qubits | Amplitudes | Memory (complex128) |
|--------|------------|---------------------|
| 20 | ~1M | 16 MB |
| 30 | ~1B | 16 GB |
| 40 | ~1T | 16 TB |
| 50 | ~1P | 16 PB |

## 3.2 Quantum Feature Maps

### 3.2.1 Definition

A quantum feature map encodes classical data into quantum states:

```
Ï†: ğ’³ â†’ â„‹
Ï†(x) = U(x)|0âŸ©^âŠ—n
```

where x âˆˆ ğ’³ âŠ† â„áµˆ is classical input data and U(x) is a parameterized unitary circuit.

### 3.2.2 The Hamiltonian Ansatz (Primary Implementation)

**Circuit Structure:**

```
Layer Structure (repeated r times):

1. INITIALIZATION (once): H^âŠ—n - Hadamard on all qubits

   |0âŸ© â”€[H]â”€ â†’ |+âŸ© = (|0âŸ© + |1âŸ©)/âˆš2

2. SINGLE-QUBIT ENCODING: Rz(Î³Â·xáµ¢/Ï€) on qubit i

   Applies phase rotation based on feature value

3. TWO-QUBIT ENTANGLEMENT: R_XX(Î³Â²(1-xáµ¢)(1-xâ±¼)) on pairs (i,j)

   Creates entanglement proportional to feature interaction
```

**Mathematical Formulation:**

```
U(x) = H^âŠ—n âˆ_{â„“=1}^{r} [ âˆ_{i=1}^{n} Rz(Î³xáµ¢/Ï€) âˆ_{(i,j)âˆˆE} R_XX(Î³Â²(1-xáµ¢)(1-xâ±¼)) ]
```

**Gate Definitions:**

```
         â”Œ                      â”
Rz(Î¸) =  â”‚ e^(-iÎ¸/2)     0     â”‚
         â”‚     0      e^(iÎ¸/2)  â”‚
         â””                      â”˜

            â”Œ                                              â”
R_XX(Î¸) =   â”‚ cos(Î¸/2)    0         0      -iÂ·sin(Î¸/2)   â”‚
            â”‚    0     cos(Î¸/2) -iÂ·sin(Î¸/2)     0        â”‚
            â”‚    0    -iÂ·sin(Î¸/2) cos(Î¸/2)      0        â”‚
            â”‚-iÂ·sin(Î¸/2)   0         0       cos(Î¸/2)    â”‚
            â””                                              â”˜
```

### 3.2.3 The Magic Ansatz (Alternative)

**Circuit Structure:**
```
Layer Structure (repeated r times):
1. Hadamard H on all qubits
2. T gate (Ï€/8 rotation) on all qubits
3. CZ gates on connected pairs
4. Rz(xáµ¢) encoding on qubit i
```

Creates "magic states" with quantum contextuality properties.

### 3.2.4 Entanglement Topology

**Linear Nearest-Neighbor Connectivity:**

```
E = {(i, i+1) : i âˆˆ [0, n-2]}

Example for n=5:
Qubits:  0 --- 1 --- 2 --- 3 --- 4
Pairs:   [(0,1), (2,3), (1,2), (3,4)]
```

**Rationale:**
- Matches typical quantum hardware constraints
- Sufficient for capturing local correlations
- Enables efficient MPS simulation

## 3.3 Kernel Methods in Machine Learning

### 3.3.1 The Kernel Trick

A function k: ğ’³ Ã— ğ’³ â†’ â„ is a valid kernel if it corresponds to an inner product:

```
k(x, x') = âŸ¨Ï†(x), Ï†(x')âŸ©_â„±
```

for some feature map Ï†: ğ’³ â†’ â„±.

### 3.3.2 Mercer's Theorem

A symmetric function k(x, x') is a valid kernel if and only if it is positive semi-definite:

```
Î£áµ¢â±¼ cáµ¢câ±¼k(xáµ¢, xâ±¼) â‰¥ 0
```

for all finite sets {xáµ¢} and coefficients {cáµ¢} âŠ‚ â„.

### 3.3.3 Support Vector Machines

**Dual Form (Kernel Form):**

```
max_Î± Î£áµ¢ Î±áµ¢ - (1/2)Î£áµ¢â±¼ Î±áµ¢Î±â±¼yáµ¢yâ±¼k(xáµ¢, xâ±¼)

subject to: 0 â‰¤ Î±áµ¢ â‰¤ C, Î£áµ¢ Î±áµ¢yáµ¢ = 0
```

**Decision Function:**

```
f(x) = sign(Î£áµ¢ Î±áµ¢yáµ¢k(xáµ¢, x) + b)
```

### 3.3.4 The RBF (Gaussian) Kernel

```
k_RBF(x, x') = exp(-||x - x'||Â² / 2ÏƒÂ²)
```

Corresponds to infinite-dimensional feature space.

## 3.4 Quantum Kernels

### 3.4.1 Fidelity Quantum Kernel (FQK)

Standard quantum kernel computes state overlap:

```
k_FQK(x, x') = |âŸ¨Ï†(x')|Ï†(x)âŸ©|Â² = |âŸ¨0â¿|Uâ€ (x')U(x)|0â¿âŸ©|Â²
```

**Limitations:**
- Exponentially small for distant points
- Requires full state access
- Numerical instability

### 3.4.2 Projected Quantum Kernel (PQK)

Computes kernel from reduced density matrices:

```
k_PQK(x, x') = exp(-Î± Î£áµ¢ ||Ïáµ¢(x) - Ïáµ¢(x')||Â²_F)
```

where Ïáµ¢(x) is the single-qubit reduced density matrix.

**Advantages over FQK:**

| Property | FQK | PQK |
|----------|-----|-----|
| Dimensionality | 2â¿ (full Hilbert space) | 3n (local observables) |
| Numerical stability | Exponentially small values | Well-conditioned |
| Classical simulability | Requires full state | Only local expectation values |

---

# 4. Projected Quantum Kernels

## 4.1 Mathematical Derivation

### 4.1.1 Pauli Matrices

```
     â”Œ     â”         â”Œ      â”         â”Œ      â”
X =  â”‚ 0 1 â”‚    Y =  â”‚ 0 -i â”‚    Z =  â”‚ 1  0 â”‚
     â”‚ 1 0 â”‚         â”‚ i  0 â”‚         â”‚ 0 -1 â”‚
     â””     â”˜         â””      â”˜         â””      â”˜
```

**Properties:**
- Hermitian: Pâ€  = P
- Unitary: PÂ² = I
- Traceless: Tr(P) = 0
- Eigenvalues: Â±1

### 4.1.2 Bloch Vector Representation

Any single-qubit state can be written as:

```
Ï = (1/2)(I + râƒ— Â· Ïƒâƒ—) = (1/2)(I + râ‚“X + ráµ§Y + ráµ¤Z)
```

where the Bloch vector râƒ— = (râ‚“, ráµ§, ráµ¤) has:

```
râ‚“ = âŸ¨XâŸ©,  ráµ§ = âŸ¨YâŸ©,  ráµ¤ = âŸ¨ZâŸ©
```

### 4.1.3 Frobenius Distance

For single-qubit RDMs with Bloch vectors râƒ— and sâƒ—:

```
||Ï - Ïƒ||Â²_F = (1/2)||râƒ— - sâƒ—||Â² = (1/2)[(râ‚“-sâ‚“)Â² + (ráµ§-sáµ§)Â² + (ráµ¤-sáµ¤)Â²]
```

### 4.1.4 Complete PQK Formula

**Definition:**

```
k_PQK(x, x') = exp(-Î± Â· D(x, x'))
```

where the quantum distance is:

```
D(x, x') = Î£áµ¢â‚Œâ‚â¿ 2Â·[(âŸ¨Xáµ¢âŸ©Ë£ - âŸ¨Xáµ¢âŸ©Ë£')Â² + (âŸ¨Yáµ¢âŸ©Ë£ - âŸ¨Yáµ¢âŸ©Ë£')Â² + (âŸ¨Záµ¢âŸ©Ë£ - âŸ¨Záµ¢âŸ©Ë£')Â²]
```

This equals the squared Euclidean distance in the projected feature space:

```
D(x, x') = ||Î¦(x) - Î¦(x')||Â²â‚‚
```

where:

```
Î¦(x) = (âŸ¨Xâ‚âŸ©, âŸ¨Yâ‚âŸ©, âŸ¨Zâ‚âŸ©, ..., âŸ¨Xâ‚™âŸ©, âŸ¨Yâ‚™âŸ©, âŸ¨Zâ‚™âŸ©) âˆˆ â„Â³â¿
```

## 4.2 Kernel Properties

**Theorem:** k_PQK is a valid Mercer kernel.

**Properties:**
- k_PQK(x, x) = 1 (self-similarity)
- k_PQK(x, x') = k_PQK(x', x) (symmetry)
- 0 < k_PQK(x, x') â‰¤ 1 (bounded)

## 4.3 Connection to Classical Kernels

The PQK is an RBF kernel in a quantum-derived feature space:

```
k_PQK(x, x') = k_RBF(Î¦(x), Î¦(x'); Î³ = Î±)
```

**Key difference:** The quantum circuit acts as a nonlinear feature extractor that:
1. Encodes data into quantum states
2. Creates entanglement-mediated correlations
3. Projects back to classical observables

## 4.4 Computational Complexity

### 4.4.1 Per Data Point

| Operation | Complexity | Notes |
|-----------|------------|-------|
| Circuit simulation (MPS) | O(n Â· r Â· Ï‡Â³) | Ï‡ = bond dimension |
| Expectation values | O(n Â· Ï‡Â²) | Per qubit |

### 4.4.2 Kernel Matrix

| Operation | Complexity |
|-----------|------------|
| All MPS simulations | O((N_train + N_test) Â· n Â· r Â· Ï‡Â³) |
| Kernel entries | O(N_train Â· N_test Â· n) |

### 4.4.3 MPI Parallelization

```
T(P) â‰ˆ T(1)/P  for large N
```

where P is the number of MPI processes.

---

# 5. Tensor Network Simulation

## 5.1 Matrix Product States (MPS)

### 5.1.1 Definition

An MPS represents an n-qubit state as a chain of tensors:

```
|ÏˆâŸ© = Î£_{iâ‚,...,iâ‚™} A^[1]_{iâ‚} A^[2]_{iâ‚‚} ... A^[n]_{iâ‚™} |iâ‚...iâ‚™âŸ©
```

where:
- A^[k]_{iâ‚–} is a matrix of dimensions Ï‡â‚–â‚‹â‚ Ã— Ï‡â‚–
- iâ‚– âˆˆ {0, 1} is the physical index (qubit state)
- Ï‡â‚– is the **bond dimension** at bond k

**Pictorial Representation:**

```
    iâ‚    iâ‚‚    iâ‚ƒ    iâ‚„    iâ‚…
    |     |     |     |     |
   [AÂ¹]--[AÂ²]--[AÂ³]--[Aâ´]--[Aâµ]
       Ï‡â‚   Ï‡â‚‚   Ï‡â‚ƒ   Ï‡â‚„
```

### 5.1.2 Bond Dimension and Entanglement

The bond dimension Ï‡ controls entanglement capacity:

```
S(Ï_left) â‰¤ logâ‚‚(Ï‡â‚–)
```

The entanglement entropy across a cut is bounded by log of bond dimension.

### 5.1.3 Initial State

The all-zeros state |0âŸ©^âŠ—n has trivial MPS with Ï‡ = 1 (product state).

## 5.2 Gate Application on MPS

### 5.2.1 Single-Qubit Gates

```
Ãƒ^[k]_j = Î£áµ¢ U_ji A^[k]_i
```

Complexity: O(Ï‡Â²) â€” does not increase bond dimension.

### 5.2.2 Two-Qubit Gates (Adjacent Sites)

**Algorithm:**
1. Contract neighboring tensors: Î˜ = A^[k] Â· A^[k+1]
2. Apply gate: Î˜Ìƒ = U Â· Î˜
3. Decompose via SVD: Î˜Ìƒ = U Î£ Vâ€ 
4. Truncate to maximum bond dimension

Complexity: O(Ï‡Â³) for SVD, may increase Ï‡.

### 5.2.3 Non-Adjacent Gates

For gates between non-adjacent qubits:
1. SWAP qubits until adjacent
2. Apply the gate
3. SWAP back

The pytket compiler handles this via `DecomposeBRIDGE` pass.

## 5.3 ITensor Implementation

### 5.3.1 Overview

ITensor is a C++ library for tensor network computations developed at the Flatiron Institute.

**Reference:** Fishman, M., White, S. R., & Stoudenmire, E. M. (2022). "The ITensor Software Library for Tensor Network Calculations." SciPost Physics Codebases, 4.

### 5.3.2 Gate Application Pattern (C++)

**Single-Qubit Gate:**
```cpp
psi.position(i1+1);
auto G = op(site_inds, "Rz", i1+1, {"alpha=", a});
auto new_MPS = G * psi(i1+1);
new_MPS.noPrime();
psi.set(i1+1, new_MPS);
```

**Two-Qubit Gate:**
```cpp
psi.position(i1+1);
auto wf = psi(i1+1) * psi(i2+1);
wf *= G;
wf.noPrime();
auto [U, S, V] = svd(wf, inds(psi(i1+1)), {"Cutoff=", 1E-10});
psi.set(i1+1, U);
psi.set(i2+1, S*V);
```

### 5.3.3 Expectation Value Computation

```cpp
for (int i = 0; i < no_sites; i++) {
    psi.position(i+1);
    auto scalar_x = eltC(
        dag(prime(psi.A(i+1), "Site")) *
        site_inds.op("X_half", i+1) *
        psi.A(i+1)
    ).real();
    // Similarly for Y, Z
}
```

### 5.3.4 Python-C++ Bridge

```cpp
PYBIND11_MODULE(helloitensor, m) {
    m.def("circuit_xyz_exp",
          &circuit_xyz_exp<int,double>,
          "Extract X,Y,Z expectation values from circuit simulation");
}
```

**Python Usage:**
```python
from helloitensor import circuit_xyz_exp
exp_xyz = circuit_xyz_exp(circuit_gates, n_qubits)
# Returns: [[âŸ¨Xâ‚âŸ©, âŸ¨Yâ‚âŸ©, âŸ¨Zâ‚âŸ©], [âŸ¨Xâ‚‚âŸ©, âŸ¨Yâ‚‚âŸ©, âŸ¨Zâ‚‚âŸ©], ...]
```

## 5.4 Complexity and Scalability

### 5.4.1 MPS Operations

| Operation | Complexity |
|-----------|------------|
| Initialize |0âŸ©^âŠ—n | O(n) |
| Single-qubit gate | O(Ï‡Â²) |
| Two-qubit gate (adjacent) | O(Ï‡Â³) |
| Two-qubit gate (distance d) | O(d Â· Ï‡Â³) |
| Expectation value | O(Ï‡Â²) |

### 5.4.2 When MPS Works Well

MPS simulation is efficient when:
- Circuits have linear (1D) connectivity
- Entanglement remains bounded
- Gates are local (nearest-neighbor)

### 5.4.3 Scalability Guidelines

| Qubits | Recommended Ï‡_max | Memory (per state) |
|--------|-------------------|-------------------|
| 10-20 | 50 | ~1 MB |
| 20-50 | 100 | ~10 MB |
| 50-100 | 200 | ~100 MB |
| 100+ | 500 | ~1 GB |

---

# 6. QMLOps Pipeline

## 6.1 Pipeline Stages Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           COMPLETE PIPELINE FLOW                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Stage 1: DATA INGESTION
â”œâ”€â”€ Load CSV data
â”œâ”€â”€ Handle missing values
â”œâ”€â”€ Encode class labels
â””â”€â”€ Filter unknown labels

Stage 2: PREPROCESSING
â”œâ”€â”€ QuantileTransform (handle outliers)
â”œâ”€â”€ StandardScaler (zero mean, unit variance)
â”œâ”€â”€ MinMaxScaler (scale to [-Ï€, Ï€])
â””â”€â”€ Feature selection (top k features)

Stage 3: CLASS BALANCING
â”œâ”€â”€ Stratified sampling
â”œâ”€â”€ n_illicit samples from fraud class
â”œâ”€â”€ n_licit samples from legitimate class
â””â”€â”€ Train/test split (80/20)

Stage 4: QUANTUM FEATURE EXTRACTION
â”œâ”€â”€ Build symbolic circuit (pytket)
â”œâ”€â”€ Substitute data values
â”œâ”€â”€ Compile to linear architecture
â”œâ”€â”€ Simulate with MPS (ITensor)
â””â”€â”€ Extract âŸ¨XâŸ©, âŸ¨YâŸ©, âŸ¨ZâŸ©

Stage 5: KERNEL CONSTRUCTION
â”œâ”€â”€ Compute Î¦(x) for all training points
â”œâ”€â”€ Build K_train (NÃ—N symmetric)
â”œâ”€â”€ Build K_test (MÃ—N)
â””â”€â”€ MPI parallelization

Stage 6: MODEL TRAINING
â”œâ”€â”€ SVM with precomputed kernel
â”œâ”€â”€ Grid search over C values
â””â”€â”€ Select best model

Stage 7: EVALUATION
â”œâ”€â”€ Predict on test set
â”œâ”€â”€ Compute metrics (Accuracy, Precision, Recall, F1, AUC)
â””â”€â”€ Compare with RBF baseline

Stage 8: DEPLOYMENT
â”œâ”€â”€ Save model (.pkl)
â”œâ”€â”€ Save scaler (.pkl)
â””â”€â”€ Production inference
```

## 6.2 Data Preprocessing Details

### 6.2.1 Preprocessing Pipeline

**Step 1: Quantile Transform**
```
x' = Î¦â»Â¹(F(x))
```
Maps data to Gaussian distribution, handles outliers.

**Step 2: Standardization**
```
x'' = (x' - Î¼) / Ïƒ
```
Zero mean, unit variance.

**Step 3: MinMax Scaling**
```
x''' = a + (x'' - min(x''))(b - a) / (max(x'') - min(x''))
```
Scale to [a, b] = [-Ï€, Ï€] or [-Ï€/4, Ï€/4].

### 6.2.2 Scaling Range Selection

| Qubits | Recommended Range |
|--------|-------------------|
| n < 10 | [-Ï€, Ï€] |
| 10 â‰¤ n < 20 | [-Ï€/2, Ï€/2] |
| n â‰¥ 20 | [-Ï€/4, Ï€/4] |

### 6.2.3 Class Balancing

```python
def draw_sample(df, ndmin, ndmaj, test_frac=0.2, seed=123):
    # Stratified sampling from each class
    data_reduced = pd.concat([
        df[df['Class']==0].sample(ndmin, random_state=seed*20+2),
        df[df['Class']==1].sample(ndmaj, random_state=seed*46+9)
    ])

    # Stratified train/test split
    train_df, test_df = train_test_split(
        data_reduced,
        stratify=data_reduced['Class'],
        test_size=test_frac,
        random_state=seed*26+19
    )
    return train_features, train_labels, test_features, test_labels
```

## 6.3 Circuit Construction Flow

```
Classical Feature Vector x = [xâ‚, xâ‚‚, ..., xâ‚™]
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CREATE SYMBOLIC CIRCUIT (pytket)                              â”‚
â”‚                                                                            â”‚
â”‚    Symbols: f_0, f_1, ..., f_{n-1}                                        â”‚
â”‚                                                                            â”‚
â”‚    â”Œâ”€â”€â”€â”                                                                   â”‚
â”‚    â”‚ H â”‚ on all qubits (initialization)                                   â”‚
â”‚    â””â”€â”€â”€â”˜                                                                   â”‚
â”‚         â”‚  Ã— r repetitions                                                 â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚
â”‚    â”‚ Rz(Î³Â·fáµ¢/Ï€)  â”‚ on each qubit i                                       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                        â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚    â”‚ XXPhase(Î³Â²(1-fáµ¢)(1-fâ±¼))    â”‚ on entangled pairs (i,j)               â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SYMBOL SUBSTITUTION                                           â”‚
â”‚    symbol_map = {f_0: xâ‚, f_1: xâ‚‚, ..., f_{n-1}: xâ‚™}                     â”‚
â”‚    circuit.symbol_substitution(symbol_map)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CIRCUIT COMPILATION (pytket)                                  â”‚
â”‚    â€¢ Map to linear architecture                                           â”‚
â”‚    â€¢ Decompose BRIDGE gates (for non-adjacent qubits)                     â”‚
â”‚    â€¢ Insert SWAPs as needed                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SERIALIZE TO GATE LIST                                        â”‚
â”‚    Output: [(0, 0, -1, 0), (0, 1, -1, 0), (2, 0, -1, Î¸â‚€), ...]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 6.4 MPI Parallelization Strategy

### 6.4.1 Data Distribution

```
Data X = [xâ‚, xâ‚‚, ..., xâ‚™] distributed across P processes:

Process 0: [xâ‚, ..., x_{N/P}]         â†’ Chunk Xâ‚€
Process 1: [x_{N/P+1}, ..., x_{2N/P}] â†’ Chunk Xâ‚
...
Process P-1: [x_{(P-1)N/P+1}, ..., xâ‚™] â†’ Chunk X_{P-1}
```

### 6.4.2 Round Robin Communication

```
Iteration 0:                    Iteration 1:
â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”
â”‚ P0  â”‚  â”‚ P1  â”‚  â”‚ P2  â”‚      â”‚ P0  â”‚  â”‚ P1  â”‚  â”‚ P2  â”‚
â”‚ Y0  â”‚  â”‚ Y1  â”‚  â”‚ Y2  â”‚  â†’   â”‚ Y2  â”‚  â”‚ Y0  â”‚  â”‚ Y1  â”‚
â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜
   â†“        â†“        â†“            â†“        â†“        â†“
Compute  Compute  Compute      Compute  Compute  Compute
K[0,0]   K[1,1]   K[2,2]       K[0,2]   K[1,0]   K[2,1]

Final: MPI_Reduce to sum all partial kernel matrices
```

### 6.4.3 Symmetry Exploitation

For training kernel (X = Y), K_ij = K_ji:
- Only compute upper triangle
- Copy to lower triangle
- Reduces computation by ~50%

### 6.4.4 Checkpointing

```python
if minutes_per_checkpoint is not None:
    if last_checkpoint_time + 60*minutes_per_checkpoint < MPI.Wtime():
        np.save(checkpoint_file, kernel_mat)
        last_checkpoint_time = MPI.Wtime()
```

## 6.5 Output Artifacts

| Artifact | Location | Description |
|----------|----------|-------------|
| Training Kernel | `kernels/TrainKernel_*.npy` | NÃ—N kernel matrix |
| Test Kernel | `kernels/TestKernel_*.npy` | MÃ—N kernel matrix |
| Profiling Data | `*.json` | Performance metrics |
| Checkpoints | `tmp/checkpoint_*.npy` | Recovery files |
| Trained Model | `model/*.pkl` | Serialized classifier |
| Feature Arrays | `pqf_arr/*.npy` | Quantum features |
| Results | `Result_*.csv` | Predictions and metrics |

---

# 7. Implementation Reference

## 7.1 Core Classes

### 7.1.1 ProjectedKernelStateAnsatz

**Location:** `projected_kernel_ansatz.py`

```python
class ProjectedKernelStateAnsatz:
    """
    Creates parameterized quantum circuits for projected kernel computation.

    Attributes:
        ansatz_circ (Circuit): The pytket circuit with symbolic parameters
        feature_symbol_list (List[Symbol]): Symbols f_0, f_1, ..., f_{n-1}
        reps (int): Number of circuit layer repetitions
        gamma (float): Rotation scaling parameter
        num_features (int): Number of qubits/features
        hadamard_init (bool): Whether to apply initial Hadamard layer
        entanglement_map (List[Tuple[int, int]]): Qubit connectivity
    """

    def __init__(
        self,
        num_features: int,       # Number of qubits = features
        reps: int,               # Layer repetitions
        gamma: float,            # Rotation scaling [0.1, 1.0]
        entanglement_map: List[Tuple[int, int]],
        ansatz: str,             # "hamiltonian" or "magic"
        hadamard_init: bool = True
    )

    def circuit_for_data(self, feature_values: List[float]) -> Circuit
    def circuit_to_list(self, circuit: Circuit) -> List[List]
    def hamiltonian_ansatz(self) -> int
    def magic_ansatz(self) -> int
```

### 7.1.2 Key Functions

**build_kernel_matrix:**
```python
def build_kernel_matrix(
    mpi_comm,                          # MPI communicator
    ansatz: ProjectedKernelStateAnsatz,
    X: np.ndarray,                     # Data matrix (N, d)
    Y: Optional[np.ndarray] = None,    # Optional second dataset
    alpha: float = 1,                  # Kernel bandwidth
    info_file: Optional[str] = None,   # Profiling output file
    cpu_max_mem: int = 6,              # Memory limit (GB)
    minutes_per_checkpoint: Optional[int] = None
) -> np.ndarray
```

**entanglement_graph:**
```python
def entanglement_graph(nq: int, nn: int) -> List[Tuple[int, int]]
# Example: entanglement_graph(5, 1) -> [(0,1), (2,3), (1,2), (3,4)]
```

## 7.2 Parameter Reference

### 7.2.1 Quantum Circuit Parameters

| Parameter | Symbol | Type | Range | Default | Description |
|-----------|--------|------|-------|---------|-------------|
| `num_features` | n | int | [2, 100+] | - | Number of qubits/features |
| `reps` | r | int | [1, 20] | 2 | Circuit layer repetitions |
| `gamma` | Î³ | float | (0, 1] | 1.0 | Rotation scaling factor |
| `alpha` | Î± | float | (0, 10] | 1.0 | Kernel bandwidth |
| `ansatz` | - | str | {"hamiltonian", "magic"} | "hamiltonian" | Circuit type |
| `hadamard_init` | - | bool | {True, False} | True | Initial H gates |

### 7.2.2 Data Parameters

| Parameter | Type | Range | Description |
|-----------|------|-------|-------------|
| `n_illicit` | int | [1, N/2] | Fraud class sample size |
| `n_licit` | int | [1, N/2] | Legitimate class sample size |
| `data_seed` | int | Any | Random seed for reproducibility |
| `test_frac` | float | (0, 1) | Test set fraction (default 0.2) |

### 7.2.3 SVM Parameters

| Parameter | Type | Values | Description |
|-----------|------|--------|-------------|
| `C` | float | [0.01, 2.0] | Regularization parameter |
| `kernel` | str | "precomputed" | Must be precomputed for PQK |
| `tol` | float | 1e-5 | Optimization tolerance |

### 7.2.4 Parameter Guidelines

| Scenario | Recommended Settings |
|----------|---------------------|
| Few features (n < 10) | Î³=1.0, reps=2-3, Î±=0.5-1.0 |
| Many features (n > 20) | Î³=0.3-0.5, reps=5-10, Î±=0.1-0.5 |
| Small data (N < 500) | Higher C (1.0-2.0), Î±=0.5-1.0 |
| Large data (N > 5000) | Lower C (0.1-0.5), Î±=0.1-0.3 |

## 7.3 Gate Encoding Specification

| Code | Gate | Qubits | Parameter | ITensor Operator |
|------|------|--------|-----------|------------------|
| 0 | H | 1 | None | `"H"` |
| 1 | Rx | 1 | angle | `"Rx"` with `alpha=angle` |
| 2 | Rz | 1 | angle | `"Rz"` with `alpha=angle` |
| 3 | XXPhase | 2 | angle | `expHermitian(XâŠ—X, -i*Î¸)` |
| 4 | ZZPhase | 2 | angle | `expHermitian(ZâŠ—Z, -i*Î¸)` |
| 5 | SWAP | 2 | None | Manual matrix |
| 6 | T | 1 | None | `"T"` |
| 7 | CZ | 2 | None | Manual matrix |

**Gate List Format:**
```python
# [code, qubit1, qubit2, parameter]
# qubit2 = -1 for single-qubit gates
example_circuit = [
    [0, 0, -1, 0],      # H on qubit 0
    [0, 1, -1, 0],      # H on qubit 1
    [2, 0, -1, 0.5],    # Rz(0.5) on qubit 0
    [3, 0, 1, 0.25],    # XXPhase(0.25) on qubits 0,1
]
```

## 7.4 File Format Specifications

### 7.4.1 Input Data (CSV)

```csv
Class,Feature 1,Feature 2,...,Feature N
0,1.234,5.678,...,9.012
1,3.456,7.890,...,1.234
```

### 7.4.2 Kernel Matrix (NumPy)

```python
# Training kernel: Shape (N_train, N_train), dtype float64
kernel_train = np.load("kernels/TrainKernel_Nf-12_r-10_g-1_Ntr-100.npy")

# Test kernel: Shape (N_test, N_train)
kernel_test = np.load("kernels/TestKernel_Nf-12_r-10_g-1_Ntr-100.npy")
```

**Filename Convention:**
```
{Type}Kernel_Nf-{num_features}_r-{reps}_g-{gamma}_Ntr-{n_train}.npy
```

### 7.4.3 Profiling JSON

```json
{
    "lenX": [800, "entries"],
    "lenY": [200, "entries"],
    "r0_circ_gen": [1.23, "seconds"],
    "r0_circ_sim": [45.67, "seconds"],
    "avg_circ_sim": [0.0571, "seconds"],
    "kernel_mat_time": [120.5, "seconds"],
    "total_time": [180.3, "seconds"]
}
```

---

# 8. Fraud Detection Application

## 8.1 Problem Domain

### 8.1.1 Fraud Detection Challenges

| Challenge | Description | QiML Solution |
|-----------|-------------|---------------|
| Class Imbalance | Fraud < 5% of transactions | Balanced sampling |
| High Dimensionality | Many features | Multi-qubit encoding |
| Complex Patterns | Non-linear interactions | Quantum entanglement |
| Adversarial Evolution | Fraudsters adapt | Kernel method robustness |
| Real-time Requirements | Sub-second decisions | Inference optimization |

### 8.1.2 Binary Classification Formulation

```
y = { 0  (Fraud / Illicit)
    { 1  (Legitimate / Licit)
```

## 8.2 The Elliptic Bitcoin Dataset

### 8.2.1 Dataset Statistics

| Attribute | Value |
|-----------|-------|
| Total transactions | 203,769 |
| Labeled transactions | 46,564 |
| Unlabeled transactions | 157,205 |
| Illicit (fraud) | 4,545 (9.8% of labeled) |
| Licit (legitimate) | 42,019 (90.2% of labeled) |
| Features per transaction | 166 |
| Time steps | 49 |

### 8.2.2 Feature Structure

**Local Features (94):** Transaction characteristics, aggregated from one-hop neighborhood

**Aggregated Features (72):** Neighborhood statistics, graph structural features

### 8.2.3 Data Preprocessing

```python
# Load raw data
feature_data = pd.read_csv('elliptic_txs_features.csv')
node_class = pd.read_csv('elliptic_txs_classes.csv')

# Encode labels: "1" = illicit â†’ 0, "2" = licit â†’ 1
node_class.loc[node_class["Class"] == "1", "Class"] = 0
node_class.loc[node_class["Class"] == "2", "Class"] = 1

# Remove unlabeled data
clean_data = feature_data.drop(np.where(node_class['Class']=='unknown')[0])
```

## 8.3 Model Configuration

### 8.3.1 Recommended Hyperparameters

```python
# Feature selection
num_features = 12      # 12-20 most important features

# Circuit parameters
reps = 10              # Deep circuit for complex patterns
gamma = 1.0            # Full rotation scaling

# Kernel parameter
alpha = 0.5            # Moderate bandwidth

# Data parameters
n_illicit = 100        # Balanced sampling
n_licit = 100
data_seed = 456

# SVM regularization
C_values = [2, 1.5, 1, 0.5, 0.1, 0.05, 0.01]
```

### 8.3.2 Hyperparameter Tuning Strategy

**Phase 1: Coarse Grid**
- num_features âˆˆ {8, 12, 16, 20}
- reps âˆˆ {2, 5, 10}
- gamma âˆˆ {0.5, 1.0}
- alpha âˆˆ {0.1, 0.5, 1.0}

**Phase 2: Fine-Tune**
- gamma âˆˆ {0.8, 0.9, 1.0, 1.1}
- alpha âˆˆ {0.3, 0.5, 0.7}
- C âˆˆ {0.01, 0.05, 0.1, 0.5, 1.0, 1.5, 2.0}

**Phase 3: Validate**
- Multiple seeds: {123, 456, 789, 101, 202}
- Report mean Â± std

## 8.4 Evaluation Metrics

### 8.4.1 Primary Metrics

| Metric | Formula | Target | Interpretation |
|--------|---------|--------|----------------|
| Recall | TP/(TP+FN) | > 0.80 | Fraud capture rate |
| Precision | TP/(TP+FP) | > 0.70 | Alert reliability |
| F1 Score | 2PR/(P+R) | > 0.75 | Balanced performance |
| AUC-ROC | Area under ROC | > 0.85 | Discrimination ability |

### 8.4.2 Confusion Matrix

```
                    Predicted
                  Fraud   Legit
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Actual Fraud  â”‚   TP    â”‚   FN    â”‚  â†’ Recall = TP/(TP+FN)
              â”‚  (Hit)  â”‚ (Miss)  â”‚
              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Actual Legit  â”‚   FP    â”‚   TN    â”‚  â†’ Specificity = TN/(TN+FP)
              â”‚ (False  â”‚(Correct â”‚
              â”‚ Alarm)  â”‚ Clear)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
              Precision = TP/(TP+FP)
```

### 8.4.3 Cost-Sensitive Analysis

| Error Type | Business Impact | Cost Ratio |
|------------|-----------------|------------|
| False Negative (Miss fraud) | Direct financial loss | 10-100x |
| False Positive (Flag legitimate) | Customer friction | 1x |

## 8.5 Expected Performance

### 8.5.1 Benchmark Results

| Model | Accuracy | Precision | Recall | F1 | AUC |
|-------|----------|-----------|--------|-----|-----|
| PQK-SVM (Hamiltonian) | 0.85-0.92 | 0.80-0.88 | 0.82-0.90 | 0.81-0.89 | 0.88-0.94 |
| RBF-SVM (Baseline) | 0.82-0.88 | 0.75-0.85 | 0.78-0.86 | 0.77-0.85 | 0.84-0.90 |
| LightGBM + QF | 0.88-0.94 | 0.85-0.92 | 0.85-0.91 | 0.85-0.91 | 0.90-0.96 |

### 8.5.2 Computational Benchmarks

| Configuration | Train Time | Inference | Memory |
|---------------|------------|-----------|--------|
| n=12, reps=10, N=200 | ~5 min | ~100 ms | ~500 MB |
| n=20, reps=10, N=200 | ~15 min | ~200 ms | ~1 GB |
| n=12, reps=10, N=1000 | ~2 hours | ~100 ms | ~2 GB |

---

# 9. Deployment Guide

## 9.1 Command-Line Usage

### 9.1.1 Kernel Mode (SVM)

```bash
mpirun -n <nodes> python main.py \
    <num_features> \
    <reps> \
    <gamma> \
    <alpha> \
    <n_illicit> \
    <n_licit> \
    <data_seed> \
    <data_file>
```

**Example:**
```bash
mpirun -n 4 python main_dlp.py 12 10 1 0.5 100 100 456 bitstrings_12_preproc.csv
```

### 9.1.2 Feature Mode (LightGBM)

```bash
mpirun -n <nodes> python main.py \
    <method: train|test|generate> \
    <train_data_info> \
    <test_data_info> \
    <target_label> \
    <train_flag: True|False>
```

## 9.2 Docker Deployment

### 9.2.1 Build

```bash
cd Installation-Script
docker build -t qiml .
```

### 9.2.2 Run

```bash
docker run \
   --env MPI_NODES=4 \
   --env NUM_FEATURES=12 \
   --env REPS=10 \
   --env GAMMA=1 \
   --env ALPHA=0.5 \
   --env N_ILLICIT=100 \
   --env N_LICIT=100 \
   --env DATA_SEED=456 \
   --env DATA_FILE=bitstrings_12_preproc.csv \
   qiml
```

## 9.3 ITensor Setup

### 9.3.1 Installation Steps

1. Install ITensor from https://itensor.org/docs.cgi?vers=cppv3&page=install
2. Copy `qubit.h` to `~/itensor/itensor/mps/sites/`
3. Add `#include "itensor/mps/sites/qubit.h"` to `~/itensor/itensor/all_mps.h`
4. Compile shared library

### 9.3.2 Compilation Commands

**Linux (GCC):**
```bash
g++ -m64 -std=c++17 -fconcepts -fPIC -c \
    -I. -I<pybind11_include> -I<itensor_path> \
    -O2 -DNDEBUG -Wall -Wno-unknown-pragmas \
    -o helloitensor.o helloitensor.cc \
    -I<python_include>

g++ -m64 -shared -std=c++17 -fconcepts -fPIC \
    helloitensor.o -o helloitensor.so \
    -L<itensor_lib> -litensor -lpthread -lblas -llapack
```

**macOS (Clang):**
```bash
clang++ -shared -undefined dynamic_lookup -std=c++17 -fPIC \
    -Wno-gcc-compat -I<pybind11_include> -I<itensor_path> \
    -O2 -DNDEBUG helloitensor.cc -o helloitensor.so \
    -L<itensor_lib> -litensor -framework Accelerate \
    -I<python_include>
```

## 9.4 Production Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRODUCTION DEPLOYMENT ARCHITECTURE                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Transaction Stream
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pre-Filter        â”‚  Fast classical rules (< 1ms)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Classical Model   â”‚  LightGBM screening (1-5ms)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ High-risk flagged
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   QiML Model        â”‚  Quantum-enhanced analysis (50-200ms)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
    Approve / Block / Review
```

## 9.5 Environment Variables

```bash
# MPI Configuration
NUM_NODES=4
OMPI_ALLOW_RUN_AS_ROOT=1

# OpenMP Configuration
OMP_NUM_THREADS=24
OMP_PROC_BIND=close
OMP_PLACES=cores

# ITensor Configuration
ITENSOR_USE_OMP=1
MKL_NUM_THREADS=4
OPENBLAS_NUM_THREADS=4

# Memory Optimization
OMP_STACKSIZE=2M
```

---

# 10. References

## 10.1 Core Papers

1. **Huang, H.-Y. et al.** (2021). "Power of data in quantum machine learning." *Nature Communications*, 12, 2631. https://www.nature.com/articles/s41467-021-22539-9

2. **HavlÃ­Äek, V. et al.** (2019). "Supervised learning with quantum-enhanced feature spaces." *Nature*, 567, 209-212. https://www.nature.com/articles/s41586-019-0980-2

3. **KÃ¼bler, J. M., Buchholz, S., & SchÃ¶lkopf, B.** (2021). "The Inductive Bias of Quantum Kernels." *NeurIPS 2021*.

4. **Fishman, M., White, S. R., & Stoudenmire, E. M.** (2022). "The ITensor Software Library for Tensor Network Calculations." *SciPost Physics Codebases*, 4. https://scipost.org/SciPostPhysCodeb.4

## 10.2 Fraud Detection Applications

5. **Heredge, J. et al.** (2023). "Quantum Multiple Kernel Learning in Financial Classification Tasks." arXiv:2312.00260. https://arxiv.org/abs/2312.00260

6. **Vasquez, A. C. et al.** (2023). "Financial Fraud Detection: A Comparative Study of Quantum Machine Learning Models." arXiv:2308.05237. https://arxiv.org/abs/2308.05237

7. **Weber, M. et al.** (2019). "Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics." *SIGKDD Workshop on Anomaly Detection in Finance*.

## 10.3 Additional Resources

8. **IBM Qiskit Documentation** - ZZFeatureMap. https://docs.quantum.ibm.com/api/qiskit/qiskit.circuit.library.ZZFeatureMap

9. **PennyLane** - Kernel-based training of quantum models. https://pennylane.ai/qml/demos/tutorial_kernel_based_training

10. **ITensor Documentation** - https://itensor.org/docs.cgi

---

# Appendix A: Quick Reference Card

## A.1 Command Line

```bash
# Kernel mode
mpirun -n 4 python main.py 12 10 1 0.5 100 100 456 data.csv

# Docker
docker build -t qiml . && docker run --env NUM_NODES=4 qiml
```

## A.2 Key Parameters

| Parameter | Typical Value | Description |
|-----------|---------------|-------------|
| num_features | 12 | Qubits/features |
| reps | 10 | Circuit depth |
| gamma | 1.0 | Rotation scaling |
| alpha | 0.5 | Kernel bandwidth |
| C | 0.1-1.0 | SVM regularization |

## A.3 Key Formulas

**PQK Kernel:**
```
k(x,x') = exp(-Î± Î£áµ¢ 2[(âŸ¨Xáµ¢âŸ©Ë£-âŸ¨Xáµ¢âŸ©Ë£')Â² + (âŸ¨Yáµ¢âŸ©Ë£-âŸ¨Yáµ¢âŸ©Ë£')Â² + (âŸ¨Záµ¢âŸ©Ë£-âŸ¨Záµ¢âŸ©Ë£')Â²])
```

**Hamiltonian Ansatz:**
```
U(x) = H^âŠ—n âˆáµ£ [ âˆáµ¢ Rz(Î³xáµ¢/Ï€) âˆ_{(i,j)} R_XX(Î³Â²(1-xáµ¢)(1-xâ±¼)) ]
```

## A.4 Expected Performance

| Metric | Target |
|--------|--------|
| Accuracy | 85-92% |
| Recall | 82-90% |
| F1 Score | 81-89% |
| AUC | 88-94% |
| Inference | <200ms |

---

**End of Document**

*Document Version 1.0 | January 2026 | Enterprise Quantum Engineering Team*
